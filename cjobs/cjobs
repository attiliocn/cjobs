#!/usr/bin/env python3

import configparser
import subprocess
import argparse
import sys
import os
import datetime
import pathlib

CJOBS_DIR = sys.path[0]
sys.path.append(CJOBS_DIR)
from tools.container_tools import is_cached_containers_file_old, show_cached_containers_file_contents, request_containers_using_rclone, parse_containers_list
from tools.scheduler_tools import get_scheduler_variables
from tools.write_job import write_job
from tools.util import get_hash_from_timestamp
from tools.config import config_cjobs_defaults

parser = argparse.ArgumentParser()

subparsers = parser.add_subparsers(required=True, dest='subparser')

parser_cluster = subparsers.add_parser('job-setup', add_help=False)
parser_cluster.add_argument("--container", required=True, help='.sif container ID. To list available containers use cjobs --list-containers')
parser_cluster.add_argument("--send-files", nargs='+', required=False, help='send files from local to remote directory')
parser_cluster.add_argument("--array", action='store_true', help='request a job array for multiple jobs')
parser_cluster.add_argument("-c", "--cores", type=int, required=True, help='number of cores (CPUs) to be allocated')
parser_cluster.add_argument("-m", "--memory-per-core", type=int, required=True, help='memory in MB per core required')
parser_cluster.add_argument("-t", "--time", type=float, required=True, help='allocation time in hours')

parser_containers = subparsers.add_parser('config', help='Create the configuration directory for cjobs and initialize the settings with default options. IMPORTANT: Please review the default options carefully and make necessary adjustments before using the program.')

parser_containers = subparsers.add_parser('listcontainers', help='list all available containers')

parser_gaussian = subparsers.add_parser('gaussian', help='setup a Gaussian calculation', parents=[parser_cluster])
parser_gaussian.add_argument('-j','--job', nargs='+', required=True, help='gaussian input file')

parser_gaussian = subparsers.add_parser('orca', help='setup a Gaussian calculation', parents=[parser_cluster])
parser_gaussian.add_argument('-j','--job', nargs='+', required=True, help='gaussian input file')

parser_xtb = subparsers.add_parser('xtb', help="setup a xTB calculation", parents=[parser_cluster])
parser_xtb.add_argument('-j','--job', nargs='+', required=True, help='coordinates file')
parser_xtb.add_argument('-f','--flags', type=str, default='', help='xTB flags')
parser_xtb.add_argument('--standalone', action='store_true', help='Swap first flag with job input for standalone usage')

parser_crest = subparsers.add_parser('crest', help="setup crest calculation", parents=[parser_cluster])
parser_crest.add_argument('-j','--job', nargs='+', required=True, help='coordinates file')
parser_crest.add_argument('-f','--flags', type=str, default='', help='crest flags')
parser_crest.add_argument('--standalone', action='store_true', help='Swap first flag with job input for standalone usage')

args = parser.parse_args()

config_dir = os.path.join(os.environ['HOME'], '.config', 'cjobs')
config_file = os.path.join(config_dir, 'cjobsrc')

if args.subparser == 'config':
    config_cjobs_defaults(config_dir, config_file)
    exit()

# read config file
config = configparser.ConfigParser()
try:
    with open(config_file) as f:
        config.read_file(f)
except FileNotFoundError:
    print('Error: Configuration file not found.')
    exit()

try:
    SCHEDULER = config['SCHEDULER']['engine']
    SCRATCH_DIR = config['SCHEDULER']['scratch dir']
    CONTAINERS_CLOUD_DIR = config['CONTAINERS']['cloud dir']
    CONTAINERS_LOCAL_DIR = config['CONTAINERS']['local dir']
    CONTAINER_MODE = config['CONTAINERS']['container mode']
    SINGULARITY_VERSION = config['CONTAINERS']['singularity version']
    SHARED_GID = config['FILE MANAGEMENT']['shared gid']
except KeyError:
    print("\nERROR: Your .cjobsrc file is outdated.\nPlease refer to the documentation to find the current version and update it accordingly.\n")
    exit()

# containers cached file
containers_avail_file = os.path.join(config_dir, 'containers.txt')
if args.subparser == 'listcontainers':
    if is_cached_containers_file_old(containers_avail_file):
        print("Updating the cached containers list")
        stdout, stderr = request_containers_using_rclone(CONTAINERS_CLOUD_DIR)
        if stderr:
            print(stderr)
            print("Could not fetch containers list from Google Drive.\nCheck your configuration file and try again")
            exit()
        else:
            containers = [i for i in stdout.split('\n')]
            containers.sort()
            with open(containers_avail_file, mode='w') as container_file:
                container_file.write("{: <15}{: <25}{: <}\n".format('Container ID', "Build Time", "File"))
                for container in containers:
                    container_data = container.replace('.sif','')
                    container_data = container_data.split('_')
                    container_file.write("{: <15}{: <25}{: <}\n".format(container_data[-1], container_data[-2], container))
            print("Update completed\n")
            show_cached_containers_file_contents(containers_avail_file)
            exit()
    else:
        print("Displaying containers from the cached file\n")
        show_cached_containers_file_contents(containers_avail_file)
        exit()

# check if container is valid
stdout, stderr = request_containers_using_rclone(CONTAINERS_CLOUD_DIR)
containers_list = [i for i in stdout.split('\n')]
containers_data = parse_containers_list(containers_list)
if args.container not in containers_data.keys():
    print(f'Invalid container. Container {args.container} not found!')
    print('Use cjobs listcontainers for a list of valid containers')
    exit()

# convert jobtime from hours to input format HH:MM:SS
duration = datetime.timedelta(hours=args.time)
totsec = duration.total_seconds()
h = totsec//3600
m = (totsec%3600) // 60
sec =(totsec%3600)%60 #just for reference
args.time = f"{int(h):02d}:{int(m):02d}:{int(sec):02d}"

# scheduler variables
scheduler_variables = get_scheduler_variables(SCHEDULER)

if args.array and len(args.job) > 1:
    # create the queue config file
    with open('queue.conf', mode='w') as f:
        for i, job in enumerate(args.job):
            f.write(f'{i+1},{job}\n')

    job_scratch_dir = f"\"{SCRATCH_DIR}/\"{scheduler_variables['job_id']}\"/\"{scheduler_variables['array_task_id']}\"\""
    job_scratch_dir_in_script = 'job_scratch_dir'
    
    job_input = f'$(awk -F "," -v ArrayTaskID=\"{scheduler_variables["array_task_id"]}\" ' + r"'$1==ArrayTaskID {print $2}' " + f'"${job_scratch_dir_in_script}"/queue.conf)'
    job_input_in_script = 'job_input'

    job_tag = f"array_{get_hash_from_timestamp()[:6]}"
    job_name = fr"$(echo ${job_input_in_script} | rev | cut -f 2- -d '.' | rev)"
    job_name_in_script = 'job_name'

    job_local_dir =f'{os.getcwd()}'
    job_local_dir_in_script = 'job_local_dir'

    write_job(
            args,
            scheduler=SCHEDULER, 
            containers_cloud_dir=CONTAINERS_CLOUD_DIR, 
            containers_local_dir=CONTAINERS_LOCAL_DIR,
            container=containers_data[args.container]['file'],
            shared_gid = SHARED_GID,
            singularity_version=SINGULARITY_VERSION,
            job_input=job_input,
            job_input_in_script=job_input_in_script,
            job_tag=job_tag,
            job_name=job_name,
            job_name_in_script=job_name_in_script,
            job_local_dir=job_local_dir,
            job_local_dir_in_script=job_local_dir_in_script,
            job_scratch_dir=job_scratch_dir,
            job_scratch_dir_in_script=job_scratch_dir_in_script
        )
else:
    for job in args.job:     
        job_input = job
        job_input_in_script = 'job_input'

        job_tag = pathlib.Path(job_input).stem
        job_name = job_tag
        job_name_in_script = 'job_name'
        
        job_local_dir =f'{os.getcwd()}'
        job_local_dir_in_script = 'job_local_dir'

        job_scratch_dir = f"\"{SCRATCH_DIR}/\"{scheduler_variables['job_id']}\"\""
        job_scratch_dir_in_script = 'job_scratch_dir'

        write_job(
            args,
            scheduler=SCHEDULER, 
            containers_cloud_dir=CONTAINERS_CLOUD_DIR, 
            containers_local_dir=CONTAINERS_LOCAL_DIR,
            container=containers_data[args.container]['file'],
            shared_gid = SHARED_GID,
            singularity_version=SINGULARITY_VERSION,
            job_input=job_input,
            job_input_in_script=job_input_in_script,
            job_tag=job_tag,
            job_name=job_name,
            job_name_in_script=job_name_in_script,
            job_local_dir=job_local_dir,
            job_local_dir_in_script=job_local_dir_in_script,
            job_scratch_dir=job_scratch_dir,
            job_scratch_dir_in_script=job_scratch_dir_in_script
        )

    
