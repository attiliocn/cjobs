#!/usr/bin/env python3

import configparser
import os
import sys

from tools.arguments import create_argument_parser
from tools.config import config_cjobs_defaults
from tools.container_tools import (is_cached_containers_file_old,
                                   parse_containers_list,
                                   request_containers_using_rclone,
                                   show_cached_containers_file_contents)
from tools.scheduler_tools import get_scheduler_variables
from tools.util import get_hash_from_timestamp, convert_hours_to_hhmmss
from tools.write_job import get_jobfile

from tools.jobdetails import JobDetails


# settings paths and files
CJOBS_DIR = sys.path[0]
sys.path.append(CJOBS_DIR)
config_dir = os.path.join(os.environ['HOME'], '.config', 'cjobs')
containers_avail_file = os.path.join(config_dir, 'containers.txt')
config_file = os.path.join(config_dir, 'cjobsrc')

# parse cli arguments
parser = create_argument_parser()
args = parser.parse_args()

# check current cjobs settings
if args.subparser == 'config':
    config_cjobs_defaults(config_dir, config_file)
    exit()
else:
    config = configparser.ConfigParser()
    try:
        with open(config_file) as f:
            config.read_file(f)
        try:
            SCHEDULER = config['SCHEDULER']['engine']
            SCRATCH_DIR = config['SCHEDULER']['scratch dir']
            USER_SCRATCH_DIR = config['SCHEDULER']['user scratch dir']
            CONTAINERS_CLOUD_DIR = config['CONTAINERS']['cloud dir']
            CONTAINERS_LOCAL_DIR = config['CONTAINERS']['local dir']
            CONTAINER_MODE = config['CONTAINERS']['container mode']
            SINGULARITY_VERSION = config['CONTAINERS']['singularity version']
            SHARED_GID = config['FILE MANAGEMENT']['shared gid']
        except KeyError:
            print(
                "ERROR: Your .cjobsrc file is outdated.\n"
                "Please refer to the documentation to find\n"
                "the current version and update it accordingly."
            )
            exit()
    except FileNotFoundError:
        print('Error: Configuration file not found.')
        exit()

# list available containers
if args.subparser == 'listcontainers':
    if is_cached_containers_file_old(containers_avail_file):
        print("Updating the cached containers list")
        stdout, stderr = request_containers_using_rclone(CONTAINERS_CLOUD_DIR)
        if stderr:
            print(stderr)
            print("Could not fetch containers list from Google Drive.\nCheck your configuration file and try again")
            exit()
        else:
            containers = [i for i in stdout.split('\n')]
            containers.sort()
            with open(containers_avail_file, mode='w') as container_file:
                container_file.write("{: <15}{: <25}{: <}\n".format('Container ID', "Build Time", "File"))
                for container in containers:
                    container_data = container.replace('.sif','')
                    container_data = container_data.split('_')
                    container_file.write("{: <15}{: <25}{: <}\n".format(container_data[-1], container_data[-2], container))
            print("Update completed\n")
            show_cached_containers_file_contents(containers_avail_file)
            exit()
    else:
        print("Displaying containers from the cached file\n")
        show_cached_containers_file_contents(containers_avail_file)
        exit()

# check if the provided container is valid
stdout, stderr = request_containers_using_rclone(CONTAINERS_CLOUD_DIR)
containers_list = [i for i in stdout.split('\n')]
containers_data = parse_containers_list(containers_list)
if args.container not in containers_data.keys():
    print(
        f'Invalid container. Container {args.container} not found.\n'
        'Use \'cjobs listcontainers\' for a list of valid containers'
    )
    exit()

# get scheduler variables
scheduler_variables = get_scheduler_variables(SCHEDULER)

# parse job information to object
jobs = JobDetails(args.jobs)
jobs.scheduler = SCHEDULER
jobs.software = args.subparser
jobs.container = containers_data[args.container]['file']
jobs.mode = args.mode
if jobs.mode == 'single':
    jobs.isArray = False
    jobs.isMassive = False
elif jobs.mode == 'array':
    jobs.isArray = True
    jobs.isMassive = False
elif jobs.mode == 'massive':
    jobs.isArray = False
    jobs.isMassive = True
jobs.cpu = args.cores
jobs.ram = args.memory_per_core
jobs.time = convert_hours_to_hhmmss(args.time)
jobs.singularity_version = SINGULARITY_VERSION
jobs.GID = SHARED_GID

if hasattr(args, "flags"):
    jobs.options = args.flags

if hasattr(args, "standalone"):
    jobs.standalone = args.standalone

#jobs.cjobsID = get_hash_from_timestamp()[:4]
jobs.cjobsID = 'abc1'

if jobs.isArray:
    jobs.numJobs = 1
    jobs.bashJobname = f"$(get_csv_element cjobs_joblist.csv \"{scheduler_variables['array_task_id']}\" 2)"
else:
    jobs.bashJobname = '$(get_csv_element cjobs_joblist.csv "$job_number" 2)'    

jobs.localDir = f"{os.getcwd()}"
jobs.scrDir = f"{SCRATCH_DIR}"
jobs.usrScrDir = f"{USER_SCRATCH_DIR}"
jobs.exeDir = f"{jobs.usrScrDir}/jobs/{scheduler_variables['job_id']}"
jobs.ctDir_remote = CONTAINERS_CLOUD_DIR
jobs.ctDir_local = CONTAINERS_LOCAL_DIR

if args.send_files:
    jobs.sendAdditionalFiles = True
    for f in args.send_files:
        jobs.include_additional_file(f)

if hasattr(args, "use_reference"):
    if args.use_reference:
        jobs.sendAdditionalFiles = True
        jobs.include_additional_file('"$basename".ref', rename='coord.ref')

if hasattr(args, "use_input"):
    if args.use_input:
        jobs.sendAdditionalFiles = True
        jobs.include_additional_file('"$basename".inp', rename=f'{jobs.software}.inp')

# write joblist
jobs.write_joblist('cjobs_joblist.csv')

# write jobfiles
jobfile = get_jobfile(jobs)
with open(f'cjobs_{jobs.cjobsID}.sh', mode='w') as f:
    f.write(jobfile)

    
